{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1yGcJYvMorAV5npEzdt94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aebongbing/ESAA/blob/main/ESAA_0509_%ED%95%84%EC%82%AC_%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09. 추천 시스템\n",
        "\n",
        "1. 콘테츠 기반 필터링 추천 시스템\n",
        "- 사용자가 특정한 아이템을 매우 선호하는 경우, 그 아이템과 비슷한 콘텐츠를 가진 다른 아이템을 추천하는 방식\n",
        "\n",
        " ex) 사용자가 특정 영화에 높은 평점을 줬다면 그 영화의 장르, 출연 배우, 감독, 영화 키워드 등의 콘텐츠와 유사한 다른 영화를 추천해주는 방식\n",
        "\n",
        "2. 협업 필터링\n",
        "\n",
        "   - 사용자가 아이템에 매긴 평점 정보나 상품 구매 이력과 같은 사용자 행동 양식만을 기반으로 추천을 수행하는 것\n",
        "\n",
        "   - 주요 목표: 사용자-아이템 평점 매트릭스와 같은 축적된 사용자 행동 데이터를 기반으로 사용자가 아직 평가하지 않은 아이템을 예측 평가하는 것\n",
        "\n",
        "   - 최근접 이웃 방식과 잠재 요인 방식으로 나뉘며 두 방식 모두 사용자-아이템 평점 행렬 데이터에만 의지해 추천을 수행함, 사용자-아이템 평점 행렬에서 행은 개별 사용자, 열은 개별 아이템으로 구성되며 사용자 아이디 행, 아이템 아이디 열 위치에 해당하는 값이 평점을 나타내는 형태가 되어야함, 일반적으로 이러한 사용자-아이템 평점 행렬은 많은 아이템을 열로 가지는 다차원 행렬이며, 사용자가 아이템에 대한 평점을 매기는 경우가 많지 않기 때문에 희소 행렬 특성을 가지고 있음\n",
        "\n",
        "   - 최근접 이웃 협업 필터링\n",
        "    \n",
        "     - 사용자 기반: 특정 사용자와 유사한 다른 사용자를 TOP-N으로 선정해 이 TOP-N 사용자가 좋아하는 아이템을 추천하는 방식\n",
        "\n",
        "     - 아이템 기반:사용자들이 그 아이템을 좋아하는지/ 싫어하는지의 평가 척도가 유사한 아이템을 추천하는 기준이 되는 알고리\n",
        "\n",
        "     - 일반적으로 아이템 기반 협업 필터링이 정확도가 더 높음\n",
        "\n",
        "   - 잠재 요인 협업 필터링\n",
        "\n",
        "     - 대규모 다차원 행렬을 차원 감소 기법으로 분해하는 과정(행렬 분해)에서 잠재적 요인을 추출해 추천 예측을 할 수 있게 하는 기법\n",
        "\n",
        "     - 사용자-아이템 평점 행렬 데이터만을  이용함\n",
        "\n",
        "     - 알고리즘 골자: 다차원 희소 행렬인 사용자-아이템 행렬 데이터를 저차원 밀집 행렬의 사용자-잠재요인 행렬과 잠재요인-아이템 행렬로 분해할 수 있으며, 이렇게 분해된 두 행렬의 내적을 통해 새로운 예측 사용자-아이템 평점 행렬 데이터를 만들어서 사용자가 아직 평점을 부여하지 않는 아이템에 대한 예측 평점을 생성하는 것\n",
        "\n",
        "     \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "M4rCimLP6rad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DzMoIr8R5jAQ"
      },
      "outputs": [],
      "source": [
        "#SGD를 이용해 행렬 분해를 수행하는 예제\n",
        "#분해하려는 원본 행렬 R을 P와 Q로 분해한 뒤에 다시 P와 Q.T의 내적으로 예측 행렬을 만드는 예제\n",
        "#원본 행렬 R을 미정인 널 값을 포함해 생성하고 분해 행렬 P와 Q는 정규 분포를 가진 랜덤 값으로 초기화\n",
        "#잠재요인 차원은 3으로 설정\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "R=np.array([[4, np.NaN, np.NaN, 2, np.NaN],\n",
        "            [np.NaN, 5, np.NaN, 3, 1],\n",
        "            [np.NaN, np.NaN, 3, 4, 4],\n",
        "            [5, 2, 1, 2,np.NaN]])\n",
        "num_users, num_items=R.shape\n",
        "k=3\n",
        "\n",
        "np.random.seed(1)\n",
        "P=np.random.normal(scale=1./k,size=(num_users, k))\n",
        "Q=np.random.normal(scale=1./k,size=(num_items, k))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def get_rmse(R,P,Q,non_zeros):\n",
        "    error = 0\n",
        "    # 두 개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성\n",
        "    full_pred_matrix = np.dot(P,Q.T)\n",
        "\n",
        "    #실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출해 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
        "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
        "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
        "    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
        "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
        "    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "D43hT-iaE7Ut"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R >0 인 행 위치, 열 위치, 값을 non_zeros 리스트에 저장\n",
        "non_zeros=[(i,j,R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j]>0]\n",
        "\n",
        "steps=1000\n",
        "learning_rate=0.01\n",
        "r_lambda=0.01\n",
        "\n",
        "#SGD 기법으로 P와 Q 매트릭스를 계속 업데이트\n",
        "for step in range(steps):\n",
        "  for i, j , r in non_zeros:\n",
        "    #실제 값과 예측 값의 차이인 오류 값 구함\n",
        "    eij=r-np.dot(P[i,:],Q[j,:].T)\n",
        "    #Regularization을 반영한 SGD 업데이트 공식 적용\n",
        "    P[i,:]=P[i,:]+learning_rate*(eij*Q[j,:]-r_lambda*P[i,:])\n",
        "    Q[j,:]=Q[j,:]+learning_rate*(eij*P[i,:]-r_lambda*Q[j,:])\n",
        "    rmse=get_rmse(R, P, Q, non_zeros)\n",
        "    if (step % 50)==0:\n",
        "      print(\"### iteration step:\",step,\"rmse:\",rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkReebfnELGz",
        "outputId": "2c572279-c103-425a-b46d-f98c1737b38f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### iteration step: 0 rmse: 3.2475255402584495\n",
            "### iteration step: 0 rmse: 3.2465908465596782\n",
            "### iteration step: 0 rmse: 3.240001170538425\n",
            "### iteration step: 0 rmse: 3.2381485790985485\n",
            "### iteration step: 0 rmse: 3.2383355624029324\n",
            "### iteration step: 0 rmse: 3.237066663018253\n",
            "### iteration step: 0 rmse: 3.235237278782524\n",
            "### iteration step: 0 rmse: 3.2334055187092163\n",
            "### iteration step: 0 rmse: 3.228120562324444\n",
            "### iteration step: 0 rmse: 3.22631720731668\n",
            "### iteration step: 0 rmse: 3.226425162809075\n",
            "### iteration step: 0 rmse: 3.225031693566006\n",
            "### iteration step: 50 rmse: 0.492114962541484\n",
            "### iteration step: 50 rmse: 0.49188671936595735\n",
            "### iteration step: 50 rmse: 0.4908767627014268\n",
            "### iteration step: 50 rmse: 0.490706148990846\n",
            "### iteration step: 50 rmse: 0.49080798389226865\n",
            "### iteration step: 50 rmse: 0.4901647116343035\n",
            "### iteration step: 50 rmse: 0.49024508668575006\n",
            "### iteration step: 50 rmse: 0.489756757805949\n",
            "### iteration step: 50 rmse: 0.48218311207525344\n",
            "### iteration step: 50 rmse: 0.4811970645881633\n",
            "### iteration step: 50 rmse: 0.48081064659815753\n",
            "### iteration step: 50 rmse: 0.47974466801926996\n",
            "### iteration step: 100 rmse: 0.16194061268842533\n",
            "### iteration step: 100 rmse: 0.16166562699344944\n",
            "### iteration step: 100 rmse: 0.16161141200643958\n",
            "### iteration step: 100 rmse: 0.16116356114982758\n",
            "### iteration step: 100 rmse: 0.16118459890224787\n",
            "### iteration step: 100 rmse: 0.1611576535414587\n",
            "### iteration step: 100 rmse: 0.16074062074907097\n",
            "### iteration step: 100 rmse: 0.1607870555862092\n",
            "### iteration step: 100 rmse: 0.16012047574106844\n",
            "### iteration step: 100 rmse: 0.16000216979571713\n",
            "### iteration step: 100 rmse: 0.15977861518477568\n",
            "### iteration step: 100 rmse: 0.15928672995049395\n",
            "### iteration step: 150 rmse: 0.07848601015141998\n",
            "### iteration step: 150 rmse: 0.07848079177619763\n",
            "### iteration step: 150 rmse: 0.0784719892990815\n",
            "### iteration step: 150 rmse: 0.07818883872608226\n",
            "### iteration step: 150 rmse: 0.0782281051577437\n",
            "### iteration step: 150 rmse: 0.07826580911164163\n",
            "### iteration step: 150 rmse: 0.07793327320473388\n",
            "### iteration step: 150 rmse: 0.0779743392550697\n",
            "### iteration step: 150 rmse: 0.07765182173568554\n",
            "### iteration step: 150 rmse: 0.07766626272528494\n",
            "### iteration step: 150 rmse: 0.0776647437457732\n",
            "### iteration step: 150 rmse: 0.07755208594596055\n",
            "### iteration step: 200 rmse: 0.045859319390486174\n",
            "### iteration step: 200 rmse: 0.04595201456054542\n",
            "### iteration step: 200 rmse: 0.04593818530380855\n",
            "### iteration step: 200 rmse: 0.04571925476884171\n",
            "### iteration step: 200 rmse: 0.045773362449762926\n",
            "### iteration step: 200 rmse: 0.04583324313321955\n",
            "### iteration step: 200 rmse: 0.04554336987382356\n",
            "### iteration step: 200 rmse: 0.04556733796254387\n",
            "### iteration step: 200 rmse: 0.04533114677976217\n",
            "### iteration step: 200 rmse: 0.04535800601049744\n",
            "### iteration step: 200 rmse: 0.04536046271008461\n",
            "### iteration step: 200 rmse: 0.04547016957963867\n",
            "### iteration step: 250 rmse: 0.030932377782945535\n",
            "### iteration step: 250 rmse: 0.03106556319026272\n",
            "### iteration step: 250 rmse: 0.031029680236964705\n",
            "### iteration step: 250 rmse: 0.030850452213331884\n",
            "### iteration step: 250 rmse: 0.030920977364975646\n",
            "### iteration step: 250 rmse: 0.03098581032875781\n",
            "### iteration step: 250 rmse: 0.030715238635107337\n",
            "### iteration step: 250 rmse: 0.030724131713940422\n",
            "### iteration step: 250 rmse: 0.030500713481216805\n",
            "### iteration step: 250 rmse: 0.030543488996828308\n",
            "### iteration step: 250 rmse: 0.030546149280102443\n",
            "### iteration step: 250 rmse: 0.030756357153356218\n",
            "### iteration step: 300 rmse: 0.023700921067176323\n",
            "### iteration step: 300 rmse: 0.02384764270710228\n",
            "### iteration step: 300 rmse: 0.023783826884411885\n",
            "### iteration step: 300 rmse: 0.02363407367817049\n",
            "### iteration step: 300 rmse: 0.02371977932294295\n",
            "### iteration step: 300 rmse: 0.023779296626743093\n",
            "### iteration step: 300 rmse: 0.02352363835476227\n",
            "### iteration step: 300 rmse: 0.023522032118347563\n",
            "### iteration step: 300 rmse: 0.023282850114225918\n",
            "### iteration step: 300 rmse: 0.023348382380318916\n",
            "### iteration step: 300 rmse: 0.02336857260102411\n",
            "### iteration step: 300 rmse: 0.023619009646156073\n",
            "### iteration step: 350 rmse: 0.02021788686228754\n",
            "### iteration step: 350 rmse: 0.020363417196870737\n",
            "### iteration step: 350 rmse: 0.02027498060491399\n",
            "### iteration step: 350 rmse: 0.020148040838373238\n",
            "### iteration step: 350 rmse: 0.02024476781930582\n",
            "### iteration step: 350 rmse: 0.020294129665721673\n",
            "### iteration step: 350 rmse: 0.020054451246922195\n",
            "### iteration step: 350 rmse: 0.02004616515150986\n",
            "### iteration step: 350 rmse: 0.019785450754631933\n",
            "### iteration step: 350 rmse: 0.019872163671029952\n",
            "### iteration step: 350 rmse: 0.019913056935359656\n",
            "### iteration step: 350 rmse: 0.020175740244585787\n",
            "### iteration step: 400 rmse: 0.018555306884096033\n",
            "### iteration step: 400 rmse: 0.018693172278321794\n",
            "### iteration step: 400 rmse: 0.018587273017947326\n",
            "### iteration step: 400 rmse: 0.018476868489610204\n",
            "### iteration step: 400 rmse: 0.018580228034010524\n",
            "### iteration step: 400 rmse: 0.018619459147895458\n",
            "### iteration step: 400 rmse: 0.018395150344702304\n",
            "### iteration step: 400 rmse: 0.018382815535496345\n",
            "### iteration step: 400 rmse: 0.018104487212290434\n",
            "### iteration step: 400 rmse: 0.018206500521417007\n",
            "### iteration step: 400 rmse: 0.01826325414603731\n",
            "### iteration step: 400 rmse: 0.01852721694884619\n",
            "### iteration step: 450 rmse: 0.017749125388039924\n",
            "### iteration step: 450 rmse: 0.01787799283775714\n",
            "### iteration step: 450 rmse: 0.017760993203808688\n",
            "### iteration step: 450 rmse: 0.017661712140436794\n",
            "### iteration step: 450 rmse: 0.01776873713301934\n",
            "### iteration step: 450 rmse: 0.017800066274960766\n",
            "### iteration step: 450 rmse: 0.017588398454789368\n",
            "### iteration step: 450 rmse: 0.017573576389943148\n",
            "### iteration step: 450 rmse: 0.017282872941760816\n",
            "### iteration step: 450 rmse: 0.017394659183730638\n",
            "### iteration step: 450 rmse: 0.017461670173759512\n",
            "### iteration step: 450 rmse: 0.01772417292263913\n",
            "### iteration step: 500 rmse: 0.017342489071125986\n",
            "### iteration step: 500 rmse: 0.017463195099101516\n",
            "### iteration step: 500 rmse: 0.017339368559432536\n",
            "### iteration step: 500 rmse: 0.01724729986303076\n",
            "### iteration step: 500 rmse: 0.01735635737519613\n",
            "### iteration step: 500 rmse: 0.0173821727449377\n",
            "### iteration step: 500 rmse: 0.017179997816739904\n",
            "### iteration step: 500 rmse: 0.017163548292492503\n",
            "### iteration step: 500 rmse: 0.016864394575987648\n",
            "### iteration step: 500 rmse: 0.016982189554233695\n",
            "### iteration step: 500 rmse: 0.017055459117684496\n",
            "### iteration step: 500 rmse: 0.017316621231486886\n",
            "### iteration step: 550 rmse: 0.01712517021043722\n",
            "### iteration step: 550 rmse: 0.017239088972313413\n",
            "### iteration step: 550 rmse: 0.017111004091682435\n",
            "### iteration step: 550 rmse: 0.017023627587490334\n",
            "### iteration step: 550 rmse: 0.01713388187564012\n",
            "### iteration step: 550 rmse: 0.01715601919034613\n",
            "### iteration step: 550 rmse: 0.016960718012420606\n",
            "### iteration step: 550 rmse: 0.016943117137424267\n",
            "### iteration step: 550 rmse: 0.016638084823766382\n",
            "### iteration step: 550 rmse: 0.016759615321581895\n",
            "### iteration step: 550 rmse: 0.01683668886149307\n",
            "### iteration step: 550 rmse: 0.017097144989823006\n",
            "### iteration step: 600 rmse: 0.016999555364021146\n",
            "### iteration step: 600 rmse: 0.017107988623161197\n",
            "### iteration step: 600 rmse: 0.01697712541287044\n",
            "### iteration step: 600 rmse: 0.01689291588773487\n",
            "### iteration step: 600 rmse: 0.017003942143965063\n",
            "### iteration step: 600 rmse: 0.017023648404998704\n",
            "### iteration step: 600 rmse: 0.016833301912645898\n",
            "### iteration step: 600 rmse: 0.016814828044551908\n",
            "### iteration step: 600 rmse: 0.016505574264468404\n",
            "### iteration step: 600 rmse: 0.016629524842347324\n",
            "### iteration step: 600 rmse: 0.016708971314448352\n",
            "### iteration step: 600 rmse: 0.016969184399783923\n",
            "### iteration step: 650 rmse: 0.016918974581069533\n",
            "### iteration step: 650 rmse: 0.01702299834808739\n",
            "### iteration step: 650 rmse: 0.016890196341032282\n",
            "### iteration step: 650 rmse: 0.016808261987279537\n",
            "### iteration step: 650 rmse: 0.016919840297588532\n",
            "### iteration step: 650 rmse: 0.016937919964934173\n",
            "### iteration step: 650 rmse: 0.016751188214482397\n",
            "### iteration step: 650 rmse: 0.016732016001539864\n",
            "### iteration step: 650 rmse: 0.01641963202055113\n",
            "### iteration step: 650 rmse: 0.016545251400206015\n",
            "### iteration step: 650 rmse: 0.016626249653981618\n",
            "### iteration step: 650 rmse: 0.01688643001680782\n",
            "### iteration step: 700 rmse: 0.016860404573811775\n",
            "### iteration step: 700 rmse: 0.0169608705553033\n",
            "### iteration step: 700 rmse: 0.016826602726679164\n",
            "### iteration step: 700 rmse: 0.016746432828981896\n",
            "### iteration step: 700 rmse: 0.016858446688334947\n",
            "### iteration step: 700 rmse: 0.01687540889151472\n",
            "### iteration step: 700 rmse: 0.016691366723842763\n",
            "### iteration step: 700 rmse: 0.016671613306581415\n",
            "### iteration step: 700 rmse: 0.016356841299460006\n",
            "### iteration step: 700 rmse: 0.016483701627880064\n",
            "### iteration step: 700 rmse: 0.016565782255103325\n",
            "### iteration step: 700 rmse: 0.01682595809387434\n",
            "### iteration step: 750 rmse: 0.016812234555127748\n",
            "### iteration step: 750 rmse: 0.016909810249468537\n",
            "### iteration step: 750 rmse: 0.01677434184450156\n",
            "### iteration step: 750 rmse: 0.01669564808037022\n",
            "### iteration step: 750 rmse: 0.016808034507313472\n",
            "### iteration step: 750 rmse: 0.016824200950734662\n",
            "### iteration step: 750 rmse: 0.016642204737009305\n",
            "### iteration step: 750 rmse: 0.016621953094516435\n",
            "### iteration step: 750 rmse: 0.016305314925454065\n",
            "### iteration step: 750 rmse: 0.016433172359986557\n",
            "### iteration step: 750 rmse: 0.01651606704620088\n",
            "### iteration step: 750 rmse: 0.0167761629468921\n",
            "### iteration step: 800 rmse: 0.01676857158858487\n",
            "### iteration step: 800 rmse: 0.016863781897757814\n",
            "### iteration step: 800 rmse: 0.016727261134303218\n",
            "### iteration step: 800 rmse: 0.01664988239092493\n",
            "### iteration step: 800 rmse: 0.01676260676079035\n",
            "### iteration step: 800 rmse: 0.016778182728572684\n",
            "### iteration step: 800 rmse: 0.01659777694424902\n",
            "### iteration step: 800 rmse: 0.016577088349542596\n",
            "### iteration step: 800 rmse: 0.016258962724825646\n",
            "### iteration step: 800 rmse: 0.0163876778083109\n",
            "### iteration step: 800 rmse: 0.01647123294707058\n",
            "### iteration step: 800 rmse: 0.016731125385411605\n",
            "### iteration step: 850 rmse: 0.01672646902886454\n",
            "### iteration step: 850 rmse: 0.01681973018240331\n",
            "### iteration step: 850 rmse: 0.016682239872119805\n",
            "### iteration step: 850 rmse: 0.016606086540773696\n",
            "### iteration step: 850 rmse: 0.016719129482484028\n",
            "### iteration step: 850 rmse: 0.016734248449789796\n",
            "### iteration step: 850 rmse: 0.01655510369474579\n",
            "### iteration step: 850 rmse: 0.016534024792447262\n",
            "### iteration step: 850 rmse: 0.016214694570497224\n",
            "### iteration step: 850 rmse: 0.016344186816629795\n",
            "### iteration step: 850 rmse: 0.016428314081843066\n",
            "### iteration step: 850 rmse: 0.016687864808158027\n",
            "### iteration step: 900 rmse: 0.016684522294856334\n",
            "### iteration step: 900 rmse: 0.016776167534416166\n",
            "### iteration step: 900 rmse: 0.016637754432690333\n",
            "### iteration step: 900 rmse: 0.016562776286994402\n",
            "### iteration step: 900 rmse: 0.01667612667253858\n",
            "### iteration step: 900 rmse: 0.016690877761128327\n",
            "### iteration step: 900 rmse: 0.01651275040955932\n",
            "### iteration step: 900 rmse: 0.016491317497517203\n",
            "### iteration step: 900 rmse: 0.016170999468386123\n",
            "### iteration step: 900 rmse: 0.016301221426625403\n",
            "### iteration step: 900 rmse: 0.016385869448168916\n",
            "### iteration step: 900 rmse: 0.016644943574669904\n",
            "### iteration step: 950 rmse: 0.016642135247517082\n",
            "### iteration step: 950 rmse: 0.01673243385410085\n",
            "### iteration step: 950 rmse: 0.01659312530882599\n",
            "### iteration step: 950 rmse: 0.01651929319425423\n",
            "### iteration step: 950 rmse: 0.01663294431422659\n",
            "### iteration step: 950 rmse: 0.01664738905536216\n",
            "### iteration step: 950 rmse: 0.016470094996700123\n",
            "### iteration step: 950 rmse: 0.016448336770160735\n",
            "### iteration step: 950 rmse: 0.016127200897529694\n",
            "### iteration step: 950 rmse: 0.016258123339566697\n",
            "### iteration step: 950 rmse: 0.01634326145454522\n",
            "### iteration step: 950 rmse: 0.016601736188367123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#P와 Q함수를 P*Q.T로 예측 행렬을 만들어서 출력\n",
        "pred_matrix=np.dot(P, Q.T)\n",
        "print('예측 행렬:\\n', np.round(pred_matrix, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao5pziMXGsb9",
        "outputId": "47be18bb-df72-4ed8-a581-aeb3e1b53d13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 행렬:\n",
            " [[3.99  0.804 1.334 2.002 1.714]\n",
            " [6.67  4.978 0.962 2.98  1.003]\n",
            " [6.843 0.408 2.987 3.977 3.986]\n",
            " [4.968 2.005 1.007 2.018 1.158]]\n"
          ]
        }
      ]
    }
  ]
}